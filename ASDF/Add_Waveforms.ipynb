{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Waveforms\n",
    "Here we combine the experiment-specific stationXML file created in GLNN_StationXML.ipynb with the recorded AE waveforms from TranAX in a tpc5 file to initialize the experiment ASDF file.\n",
    "\n",
    "ElSys has provided a module tpc5.py __[available on their website](https://www.elsys-instruments.com/en/support/tpc5_fileformat.php)__ for easy interpretation.\n",
    "\n",
    "At the bottom, I also add the TDMS waveforms for my load transducers recorded with LabView."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tpc5 # get from ElSys, see link above\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pyasdf\n",
    "from obspy import Stream, Trace\n",
    "from obspy.core.util import AttribDict\n",
    "from obspy.core import Stats\n",
    "import obspy\n",
    "from nptdms import TdmsFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First set up the ASDF dataset to fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds = pyasdf.ASDFDataSet('020918_shear/020918ASDF.h5')\n",
    "# ds = pyasdf.ASDFDataSet('020918_shear/020918ASDF.h5', compression='gzip-3')\n",
    "#ds.add_stationxml('020918_shear/GLNNstations_020918.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L0.AE09',\n",
       " 'L0.AE11',\n",
       " 'L0.AE16',\n",
       " 'L0.AE17',\n",
       " 'L0.AE18',\n",
       " 'L0.AE21',\n",
       " 'L0.AE22',\n",
       " 'L0.AE23',\n",
       " 'L0.AE27',\n",
       " 'L0.AE28',\n",
       " 'L0.AE32',\n",
       " 'L0.AE33',\n",
       " 'L0.AE35',\n",
       " 'L0.AE38',\n",
       " 'L0.AE41',\n",
       " 'L0.AE43']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.waveforms.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Station AE09 (BP3)\n",
       "\tStation Code: AE09\n",
       "\tChannel Count: 1/None (Selected/Total)\n",
       "\tNone - \n",
       "\tAccess: None \n",
       "\tLatitude: 37.87, Longitude: -122.26, Elevation: 100.0 m\n",
       "\tAvailable Channels:\n",
       "\t\tAE09.00.FHZ"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.waveforms.L0_AE09.StationXML.networks[0].stations[0]\n",
    "# ASDF appears to dump the extra attribute of the StationXML\n",
    "# Add the (x,y) locations as auxiliary data instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stations are ordered as A1-D4 in the original StationXML but get re-sorted to increasing station codes when imported to ASDF. The ASDF format also drops the .extra attribute from the StationXML, which contained my local_location information. I need to temporarily access the StationXML outside of ASDF to bring back this lost information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inv = obspy.read_inventory('020918_shear/GLNNstations_020918.xml', format='stationxml')\n",
    "ordered_stations = [inv[0].stations[i].code for i in range(16)]\n",
    "ordered_locations = [(float(inv[0].stations[i].extra.local_location.value['x'].value),\n",
    "                      float(inv[0].stations[i].extra.local_location.value['y'].value),\n",
    "                      float(inv[0].stations[i].extra.local_location.value['z'].value))\n",
    "                     for i in range(16)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These bits of extra info are great candidates for ASDF auxiliary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(27.45232, 19.05, 0.0),\n",
       " (41.447720000000004, 19.05, 0.0),\n",
       " (34.46272, 19.05, 0.0),\n",
       " (30.957520000000002, 19.05, 0.0),\n",
       " (19.558, 16.002, 0.0),\n",
       " (20.44192, 19.05, 0.0),\n",
       " (34.29, 26.162000000000003, 0.0),\n",
       " (37.846000000000004, 22.352000000000004, 0.0),\n",
       " (30.957520000000002, 22.555200000000003, 0.0),\n",
       " (26.46172, 23.368, 0.0),\n",
       " (23.876, 22.352000000000004, 0.0),\n",
       " (13.43152, 19.05, 0.0),\n",
       " (35.45332, 14.732, 0.0),\n",
       " (30.957520000000002, 15.5448, 0.0),\n",
       " (9.9568, 19.05, 0.0),\n",
       " (23.876, 15.748000000000001, 0.0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'AE27', b'AE09', b'AE17', b'AE22', b'AE38', b'AE35', b'AE18',\n",
       "       b'AE11', b'AE21', b'AE28', b'AE32', b'AE41', b'AE16', b'AE23',\n",
       "       b'AE43', b'AE33'], dtype='|S4')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([s.encode('utf8') for s in ordered_stations])\n",
    "# h5 files can't handle the numpy unicode datatype, this is a workaround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds.add_auxiliary_data(data=np.array(ordered_locations),\n",
    "                      data_type='LabStationInfo',\n",
    "                      path='local_locations',\n",
    "                      parameters={})\n",
    "ds.add_auxiliary_data(data=np.array([s.encode('utf8') for s in ordered_stations]),\n",
    "                      data_type='LabStationInfo',\n",
    "                      path='ordered_stations',\n",
    "                      parameters={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[27.45232, 19.05   ,  0.     ],\n",
       "       [41.44772, 19.05   ,  0.     ],\n",
       "       [34.46272, 19.05   ,  0.     ],\n",
       "       [30.95752, 19.05   ,  0.     ],\n",
       "       [19.558  , 16.002  ,  0.     ],\n",
       "       [20.44192, 19.05   ,  0.     ],\n",
       "       [34.29   , 26.162  ,  0.     ],\n",
       "       [37.846  , 22.352  ,  0.     ],\n",
       "       [30.95752, 22.5552 ,  0.     ],\n",
       "       [26.46172, 23.368  ,  0.     ],\n",
       "       [23.876  , 22.352  ,  0.     ],\n",
       "       [13.43152, 19.05   ,  0.     ],\n",
       "       [35.45332, 14.732  ,  0.     ],\n",
       "       [30.95752, 15.5448 ,  0.     ],\n",
       "       [ 9.9568 , 19.05   ,  0.     ],\n",
       "       [23.876  , 15.748  ,  0.     ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.auxiliary_data.LabStationInfo.local_locations.data[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now on to importing the waveforms. An experiment might be made up of multiple files. Display the tpc5 files present in the experiment folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "020918_shear/bd1.tpc5  020918_shear/run.tpc5\r\n",
      "020918_shear/bd2.tpc5  020918_shear/timing.tpc5\r\n"
     ]
    }
   ],
   "source": [
    "!ls 020918_shear/*.tpc5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This experiment had two ball drops (bd1, bd2), a timing alignment signal (timing), and the full shear run (run). Start here (after imports) to add each tpc5 file.\n",
    "\n",
    "The timing file only has one channel (D4). This is under 00000001 in the tpc5 name hierarchy, so without any tpc5 channel name checking it gets dumped into A1 here. It was also moved, so it's not even really valid to put it in AE41. Maybe I can use AE99 as a dummy location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = h5py.File(\"020918_shear/run.tpc5\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we need to know how many blocks to read\n",
    "# all channels have the same number of blocks, use channel 1\n",
    "cg = f[tpc5.getChannelGroupName(1)]\n",
    "nblocks = len(cg['blocks'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ordered_stations = ds.auxiliary_data.LabStationInfo.ordered_stations.data\n",
    "ordered_stations = [stat.decode(\"utf-8\") for stat in ordered_stations]\n",
    "#ordered_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SPECIAL block for timing signal at temporary off-plate station\n",
    "# first build some basic Stats\n",
    "statn_stats = Stats()\n",
    "statn_stats.network = 'L0'\n",
    "statn_stats.channel = 'FHZ'\n",
    "statn_stats.location = '00'\n",
    "statn_stats.sampling_rate = 20e6\n",
    "\n",
    "# make sure times will retain full precision\n",
    "# the ElSys max precision seems to be nanoseconds\n",
    "obspy.UTCDateTime.DEFAULT_PRECISION = 9\n",
    "\n",
    "statn_stats.station = 'AE99'\n",
    "statn_stream = Stream()\n",
    "\n",
    "statn_stats.starttime = (obspy.UTCDateTime(tpc5.getStartTime(f,1)) # gives the start of the whole recording\n",
    "                         + tpc5.getTriggerTime(f,1,block=blk) # seconds from start to trigger\n",
    "                         - tpc5.getTriggerSample(f,1,block=blk)/statn_stats.sampling_rate) # seconds from trigger to block start\n",
    "        \n",
    "# get the raw voltage data\n",
    "raw = tpc5.getVoltageData(f, 1)\n",
    "# give the stats the length, otherwise it takes 0 points\n",
    "statn_stats.npts = len(raw)\n",
    "statn_stream += Trace(raw ,statn_stats)\n",
    "    \n",
    "# add the complete stream to the ASDF object\n",
    "ds.add_waveforms(statn_stream, \"timing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE27\n",
      "AE09\n",
      "AE17\n",
      "AE22\n",
      "AE38\n",
      "AE35\n",
      "AE18\n",
      "AE11\n",
      "AE21\n",
      "AE28\n",
      "AE32\n",
      "AE41\n",
      "AE16\n",
      "AE23\n",
      "AE43\n",
      "AE33\n"
     ]
    }
   ],
   "source": [
    "# read the raw data from each channel into a stream, one trace at a time\n",
    "# first build some basic Stats\n",
    "statn_stats = Stats()\n",
    "statn_stats.network = 'L0'\n",
    "statn_stats.channel = 'FHZ'\n",
    "statn_stats.location = '00'\n",
    "statn_stats.sampling_rate = 20e6\n",
    "\n",
    "# make sure times will retain full precision\n",
    "# the ElSys max precision seems to be nanoseconds\n",
    "obspy.UTCDateTime.DEFAULT_PRECISION = 9\n",
    "\n",
    "# iterate through stations, following the ordered_stations A1-D4 sort\n",
    "# the number of the (ordered) station is the TranAX channel A1-D4->1-16, here Tchan\n",
    "for Tchan,statname in enumerate(ordered_stations,1):\n",
    "    # add the station to the stats\n",
    "    statn_stats.station = statname\n",
    "    print(statname)\n",
    "    \n",
    "    # create a stream for the station\n",
    "    statn_stream = Stream()\n",
    "    \n",
    "    # iterate through continuous data segments\n",
    "    # TranAX calls these Blocks, obspy calls them Traces\n",
    "    for blk in range(1,nblocks+1):\n",
    "        # get the trace start time\n",
    "        statn_stats.starttime = (obspy.UTCDateTime(tpc5.getStartTime(f,1)) # gives the start of the whole recording\n",
    "                                + tpc5.getTriggerTime(f,1,block=blk) # seconds from start to trigger\n",
    "                                - tpc5.getTriggerSample(f,1,block=blk)/statn_stats.sampling_rate) # seconds from trigger to block start\n",
    "        \n",
    "        # get the raw voltage data\n",
    "        raw = tpc5.getVoltageData(f, Tchan, block=blk)\n",
    "        # give the stats the length, otherwise it takes 0 points\n",
    "        statn_stats.npts = len(raw)\n",
    "        statn_stream += Trace(raw ,statn_stats)\n",
    "    \n",
    "    # add the complete stream to the ASDF object\n",
    "    ds.add_waveforms(statn_stream, \"run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L0.AE09.00.FHZ__2018-02-09T22:15:02.828951__2018-02-09T22:15:02.828951__bd1',\n",
       " 'L0.AE09.00.FHZ__2018-02-09T22:17:49.024309__2018-02-09T22:17:49.024309__bd2',\n",
       " 'L0.AE09.00.FHZ__2018-02-09T22:30:51.423920__2018-02-09T22:30:51.423920__run',\n",
       " 'L0.AE09.00.FHZ__2018-02-09T22:30:59.677762__2018-02-09T22:30:59.677762__run',\n",
       " 'L0.AE09.00.FHZ__2018-02-09T22:31:33.355811__2018-02-09T22:31:33.355811__run',\n",
       " 'L0.AE09.00.FHZ__2018-02-09T22:32:38.097585__2018-02-09T22:32:38.097585__run',\n",
       " 'L0.AE09.00.FHZ__2018-02-09T22:36:00.631816__2018-02-09T22:36:00.631816__run',\n",
       " 'L0.AE09.00.FHZ__2018-02-09T22:36:00.976014__2018-02-09T22:36:00.976014__run',\n",
       " 'L0.AE09.00.FHZ__2018-02-09T22:36:00.999154__2018-02-09T22:36:00.999154__run',\n",
       " 'L0.AE09.00.FHZ__2018-02-09T22:36:01.172681__2018-02-09T22:36:01.172681__run',\n",
       " 'L0.AE09.00.FHZ__2018-02-09T22:36:01.232056__2018-02-09T22:36:01.232056__run',\n",
       " 'L0.AE09.00.FHZ__2018-02-09T22:36:01.252013__2018-02-09T22:36:01.252013__run',\n",
       " 'L0.AE09.00.FHZ__2018-02-09T22:36:01.289543__2018-02-09T22:36:01.289543__run',\n",
       " 'L0.AE09.00.FHZ__2018-02-09T22:36:01.410557__2018-02-09T22:36:01.410557__run',\n",
       " 'L0.AE09.00.FHZ__2018-02-09T22:36:01.432444__2018-02-09T22:36:01.432444__run',\n",
       " 'L0.AE09.00.FHZ__2018-02-09T22:45:59.536139__2018-02-09T22:45:59.536139__run',\n",
       " 'L0.AE09.00.FHZ__2018-02-09T22:46:00.544065__2018-02-09T22:46:00.544065__run',\n",
       " 'L0.AE09.00.FHZ__2018-02-09T22:46:00.671943__2018-02-09T22:46:00.671943__run',\n",
       " 'L0.AE09.00.FHZ__2018-02-09T22:46:00.731512__2018-02-09T22:46:00.731512__run',\n",
       " 'L0.AE09.00.FHZ__2018-02-09T22:46:00.759008__2018-02-09T22:46:00.759008__run',\n",
       " 'L0.AE09.00.FHZ__2018-02-09T22:50:04.208317__2018-02-09T22:50:04.208317__run',\n",
       " 'L0.AE09.00.FHZ__2018-02-09T22:50:13.494263__2018-02-09T22:50:13.494263__run',\n",
       " 'L0.AE09.00.FHZ__2018-02-09T22:50:13.783906__2018-02-09T22:50:13.783906__run',\n",
       " 'L0.AE09.00.FHZ__2018-02-09T22:50:13.895282__2018-02-09T22:50:13.895282__run',\n",
       " 'L0.AE09.00.FHZ__2018-02-09T22:50:14.608812__2018-02-09T22:50:14.608812__run',\n",
       " 'L0.AE09.00.FHZ__2018-02-09T22:50:15.587534__2018-02-09T22:50:15.587534__run',\n",
       " 'L0.AE09.00.FHZ__2018-02-09T22:50:15.704938__2018-02-09T22:50:15.704938__run',\n",
       " 'L0.AE09.00.FHZ__2018-02-09T22:50:15.985774__2018-02-09T22:50:15.985774__run',\n",
       " 'L0.AE09.00.FHZ__2018-02-09T22:50:16.135962__2018-02-09T22:50:16.135962__run',\n",
       " 'L0.AE09.00.FHZ__2018-02-09T22:50:16.664167__2018-02-09T22:50:16.664167__run',\n",
       " 'L0.AE09.00.FHZ__2018-02-09T22:50:16.817545__2018-02-09T22:50:16.817545__run',\n",
       " 'L0.AE09.00.FHZ__2018-02-09T22:50:17.072935__2018-02-09T22:50:17.072935__run',\n",
       " 'L0.AE09.00.FHZ__2018-02-09T22:50:17.096252__2018-02-09T22:50:17.096252__run',\n",
       " 'L0.AE09.00.FHZ__2018-02-09T22:50:17.167037__2018-02-09T22:50:17.167037__run',\n",
       " 'L0.AE09.00.FHZ__2018-02-09T22:50:17.193835__2018-02-09T22:50:17.193835__run',\n",
       " 'L0.AE09.00.FHZ__2018-02-09T22:50:17.250714__2018-02-09T22:50:17.250714__run',\n",
       " 'L0.AE09.00.FHZ__2018-02-09T22:50:17.273765__2018-02-09T22:50:17.273765__run',\n",
       " 'StationXML']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.waveforms.L0_AE09.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         network: L0\n",
       "         station: AE27\n",
       "        location: 00\n",
       "         channel: FHZ\n",
       "       starttime: 2018-02-09T22:30:51.423920384Z\n",
       "         endtime: 2018-02-09T22:30:51.423920384Z\n",
       "   sampling_rate: 20000000.0\n",
       "           delta: 5e-08\n",
       "            npts: 0\n",
       "           calib: 1.0\n",
       "         _format: ASDF\n",
       "            asdf: AttribDict({'format_version': '1.0.1', 'tag': 'raw_record'})"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.waveforms.L0_AE27.raw_record[0].stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add TDMS data\n",
    "The package npTDMS has support for DAQmx raw data, so it's very easy to add here as auxiliary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tdms = TdmsFile('020918_shear/2.9.2018 - 2.25.37 PM/Board_5_25000_Hz.tdms')\n",
    "shear_volts = tdms.object('record','PXI1Slot6/ai7').data\n",
    "normal_volts = tdms.object('record','PXI1Slot6/ai6').data\n",
    "load_start = obspy.UTCDateTime(tdms.object('record','PXI1Slot6/ai6').property('wf_start_time'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds.add_auxiliary_data(data=shear_volts,\n",
    "                      data_type='LoadData',\n",
    "                      path='shear_volts',\n",
    "                      parameters={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds.add_auxiliary_data(data=normal_volts,\n",
    "                      data_type='LoadData',\n",
    "                      path='normal_volts',\n",
    "                      parameters={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds.flush()\n",
    "ds._close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra notes\n",
    "Accessing tpc5 and hdf5 files can be a bit confusing. Here are some reminders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to view the keys of an hdf5 file:\n",
    "list(f.keys())\n",
    "\n",
    "# to then access one of the keys:\n",
    "f['key']\n",
    "\n",
    "# to control the precision of a UTCDateTime\n",
    "t = obspy.UTCDateTime(precision=9)\n",
    "# doing arithmetic creates a new UTCDateTime with the default precision!\n",
    "# change the default precision for the session\n",
    "obspy.UTCDateTime.DEFAULT_PRECISION = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "WaveformNotInFileException",
     "evalue": "Tag 'timing' not part of the data set for station 'L0.AE09'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWaveformNotInFileException\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-31170ea7d72f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# clean up a botched attempt at adding raw_records\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaveforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'L0_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__delitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timing'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mordered_stations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-31170ea7d72f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# clean up a botched attempt at adding raw_records\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaveforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'L0_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__delitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timing'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mordered_stations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/obspy/lib/python3.6/site-packages/pyasdf/utils.py\u001b[0m in \u001b[0;36m__delitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    792\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__delitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__delete_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    795\u001b[0m         \u001b[0;31m# This cannot really happen as it will be caught by the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;31m# __filter_data() method.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/obspy/lib/python3.6/site-packages/pyasdf/utils.py\u001b[0m in \u001b[0;36m__delete_data\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0mInternal\u001b[0m \u001b[0mdelete\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m         \"\"\"\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__filter_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0mh5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__hdf5_group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/obspy/lib/python3.6/site-packages/pyasdf/utils.py\u001b[0m in \u001b[0;36m__filter_data\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    844\u001b[0m                 raise WaveformNotInFileException(\n\u001b[1;32m    845\u001b[0m                     \"Tag '%s' not part of the data set for station '%s'.\" % (\n\u001b[0;32m--> 846\u001b[0;31m                         item, self._station_name))\n\u001b[0m\u001b[1;32m    847\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mWaveformNotInFileException\u001b[0m: Tag 'timing' not part of the data set for station 'L0.AE09'."
     ]
    }
   ],
   "source": [
    "# clean up a botched attempt at adding raw_records\n",
    "[ds.waveforms['L0_'+code].__delitem__('timing') for code in ordered_stations]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
